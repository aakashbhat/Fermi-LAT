\section{Choice of methods}
\lb{sec:methods}

\subsection{General methodology}
We will use data-driven approach in constructing the probabilistic catalog.
The result depends on two inputs: data used for constructing the model and the choice of the method for classification.
The data in our case will be sources in 3FGL with known classes. 
We will split the data into training and testing subsets.
For the methods, we will consider four machine learning algorithms: boosted decision trees (BDTs),  random forests (RF),
logistic regression (LG), and neural networks (NN).
The resulting probabilities of classification depend on the choice of the classification algorithm.
Although some algorithms have slightly better performance on the test sample then others,
the overall performance is relatively similar.
As a result, we will report the classification probabilities for all four algorithms in the catalog, instead of selecting the 
``best'' one.
The difference among the predictions will serve as a measure of modeling uncertainty related to the choice of the classification algorithm.

\subsection{Discussion of the choice of the classification algorithms}
{\it Decision trees}
One of the most simple and transparent algorithms for classification is a decision tree.
In this algorithm, at each step the sample is split into two subsets using one of the input features.
The choice of the feature and the separating value are determined by minimizing an objective function, such as misclassification
error, Gini index, or cross-entropy.
This method is very intuitive, since at each step the results can be described in words, 
for example, at the first step, the sources can be split in "mostly" Galactic and extragalactic by a cut on the Galactic latitude.
At the next step, the high latitude sources can be further subsplit into millisecond pulsars and other sources, buy a cut on the spectral index around 1 GeV (pulsars have a hard spectrum below a few GeV) etc.
One problem with decision trees is overfitting: if the tree is too deep, then it will pick up particular cases of the training sample, while too shallow tree would not be able to describe the data well. As a result, one needs to be very careful in selecting the depth of the tree.
This problem can be avoided if a random subset of features is used to find a division at each node. This is the basis of the RF algorithm,
where the final classification is given by an average of several trees with random subsets of features used at each node.
Another problem with the simple trees is that it can miss the classification of some subsets of data. In BDT algorithms, the final classification is given by a collection of trees, where each new tree is created by increasing the weights of misclassified samples of the previous step. 
Finally, simple trees predict classes for the data samples, while we would like to have probabilities of classes (also known as soft classification).
RF and BDT algorithms, by virtue of averaging, provided probabilities. As a result, we will use RF and BDT algorithms rather than simple trees in this paper.

Tree-based algorithms, even after averaging in RF and BDT methods, have sharp edges among domains with different probabilities.
In LR algorithm, the probabilities of classes are by construction smooth functions of features.
In particular, for two-class classification the probability of class 0, given the set of features $x$, is modeled by sigmoid (logit) function
\be
\lb{eq:logit}
p_0(x) = \frac{1}{1 + e^{m(x)}}.
\ee
The probability of class 1 is then modeled as $1 - p_0(x)$.
If $m(x)$ is a linear function of features, then the boundary between the domains, defined, e.g., as $p_0(x) = 0.5$, will be linear.
More complicated boundaries can be modeled by taking non-linear functions $m(x)$.
Unknown parameters of the function $m(x)$ are determined by maximizing the log likelihood of the model given the known classes of the data in training sample.
A nice feature of the LR method is that it, by construction, provides probabilities of classes with smooth transitions among domains of different classes.
A limitation is that the form of the probability function is limited by the sigmoid function in Equation (\ref{eq:logit}).

We notice that if $m(x)$ is a linear function of features $x$, then the logistic regression model is obtained by an application of sigmoid function to a linear combination of input features.
This is in fact a single layer perceptron, or a neural network without hidden layers, with several input nodes (each node corresponds to
a features) and one output node, which corresponds to $p_0(x)$.
The output value is obtained by a non-linear transformation (sigmoid) of a linear combination of features.
Neural network with several hidden layers is obtained by a sequence of nonlinear transformations of linear combinations of features.
In particular, the values in the first hidden layer are obtained by a non-linear transformation of linear combinations of input features.
Then the values in second hidden layer are non-linear transformations of linear combinations of values in the first hidden layer etc.
In the context of neural networks, the non-linear transformations are called activation functions.
If the activation function for the output layer is sigmoid, then the output value (values) can be interpreted as probabilities.
We notice that in this case the neural network is can be expressed by a logistic regression for some function $m(x)$,
i.e., the neural network is then a particular way of constructing $m(x)$.
Thus the only difference between LR and NN for the classification problems is the construction of the function $m(x)$.
In this paper, for LR $m(x)$ will be constructed as a combination of low-order polynomials of the input features,
while for NN, $m(x)$ will be constructed by taking linear input features and several hidden layers, e.g., 4 or 5, 
in a fully connected neural network.

\subsection{Construction of a probabilistic catalog}

As an example of the construction of a probabilistic catalog, we will use with the 3FGL catalog.
For training and testing the methods, we use sources which have associations and no missing values in the catalog table.
In this paper we will perform a two-class classification to separate PS into pulsars and AGNs.
Thus, we subselect the sources, which are associated to either a pulsar or an AGN.
After the training of the algorithms, we test the performance with the test sources and predict the classes of sources without associations,
but have all features present in the catalog table.
The general workflow will have the following steps:
\ben
\item
Select data for learning and testing.
\item
Train algorithms using the learning dataset.
Tune hyper-parameters of the algorithms and test the performance on the test dataset, in particular, to avoid overfitting.
\item
Choose the most important features and
retrain the algorithms using the subset of the most important features.
\item
Make prediction for unassociated point sources of the 3FGL.
We also apply the classification for associated source. In this case we check if there are any outliers among the associated sources.
\een
As a result of the analysis in this section, we obtain a catalog with probabilistic associations of sources in 3FGL.
We will report the classification probabilities for all four algorithms and each source.
In the next section we compare the predictions in the catalog with the new 4FGL catalog.
We also construct a probabilistic catalog starting with the 4FGL.


\subsection{Data and feature selection}

We restrict attention to associated and unassociated source but without missing values. 
We use the associated sources which were classified as either AGNs (classification labels in 3FGL catalog: agn, FSRQ, fsrq, BLL, bll, BCU, bcu, RDG, rdg, NLSY1, nlsy1, ssrq, and sey) or Pulsars (classification labels in 3FGL: PSR, psr), which results in a list of 1905 sources. 
%The rest of the sources without problematic values were then used as unassociated sources, which we used later on for testing and prediction. \\
%Our methodology for classification was dependent on two things: The data that we had, which needed to be cleaned and the algorithms that we needed to apply. For this we decided on using the 3rd catalog of F-LAT (3FGL from hereon) for initial training and testing, the 4th catalog (FL8Y from hereon) for further testing and predictions.
%Our data was similar to that used by Parkinson et. al. We cleaned the 3FGL catalog to have sources which were both associated and unassociated but with no missing values.\\

There are several tens of features of point sources quoted in the catalog, such as the position, photon and energy fluxes integrated in different energy bands, spectral parameters, variability index as well as corresponding uncertainties.
In our example, we will start with 10 features:
flux density and the error on it, spectral index, spectral curvature, hardness ratios (as defined by Parkinson et. al.), variability, and the galactic latitude. 
We used the logarithmic scale for features with large spread of values, e.g., for the flux density, the error on flux, the curvature, and the variability. %The complete list of sources, along with some statistics, is given in the appendix. 
The influence of the features on the classification, especially the differences in the various methodologies is discussed in more details in the next section.
Not all of these parameters are independent, for example, log of the ratio of fluxes is proportional to the spectral index (if the spectrum is represented well by a power law) etc.
If two variables are highly correlated, then one can discard one of them, since the corresponding information can be recovered from the other one, the corresponding correlation matrix is shown in Figure \ref{fig:corr}.
In the following we will see that not all features are important for classification and further restrict our attention to the four features, which have the largest separating power for tree-based algorithms.

  

\subsection{Details of algorithms}

In this section we provide details on the training of the ML algorithms for the classification of the PS.
We will split the sources with known classifications into training (70\%) and test samples (30\%).
One of the main objectives is to increase the accuracy of classification: this is achieved by adding more parameters to the model
and tuning them using the training sample.
The test sample is used to ensure that the algorithms do not overfit the data.


One of the main aims of our project was to understand and optimize the machine learning methods which we were using. So apart from the features which were in the data itself, we also theorized and experimented with the parameters of the algorithms themselves. We wanted to find the fastest and cost-effective way of using certain methods, without going into regimes of under and over-fitting the data. Parameters which we studied range from Depth and Number of trees in Forest based methods to the number of hidden layers and epochs in neural networks. The details are given in the next section, where we discuss our expectations and the resulting behaviour of our algorithms.\\


 All of the machine learning algorithms were taken from the python module sklearn, including Neural Networks. A neural network using Keras was also attempted; however, due to the classification being on only two classes, we discarded it in favour of the sklearn algorithm which was much faster.\\
One of the main aims of our project was to understand and optimize the machine learning methods which we were using. So apart from the features which were in the data itself, we also theorized and experimented with the parameters of the algorithms themselves. We wanted to find the fastest and cost-effective way of using certain methods, without going into regimes of under and over-fitting the data. Parameters which we studied range from Depth and Number of trees in Forest based methods to the number of hidden layers and epochs in neural networks. The details are given in the next section, where we discuss our expectations and the resulting behaviour of our algorithms.\\	




\subsection{Details of the analysis}

\subsection{Data and Features}

The total number of sources, including unassociated and associated, in the two catalogs is shown below. \\
\begin{figure}[h]
%\centering
\includegraphics[width=\onepic\textwidth]{plots/correlation.pdf}
\caption{Correlation matrix for the most important features}
\label{fig:corr}
\end{figure}
[Add Table]\\

The features used for our analysis follow the same idea as the previous studies. The features, along with statistical and methodological details, are given below. A correlation matrix is presented for the most important features as well. The matrix is important for the case where there might be redundant features, in which case using only one of the two features would be a better idea.\\




Our initial hypothesis was that certain features would be more important for classification than others. For instance, as shown below, one can see a clear distinction between the regimes of AGNs and Pulsars, based on spectral idex and significant curvature. [Add image] While not clearly obvious from the get go, we were also interested in comparing the importance of features based on the algorithms that we were using. Due to the difference in the basic method of Random Forests and Neural Networks, we expected a slight shift in their reliance on certain features. Despite that we hypothesized that features with the most contribution would be among spectral index, variability, and the curvature; as already observed by Parkinson et. al. This is made clearer by the two figures below, which highlight the separation of PSR and AGN. The separation is seen to be much easier when spectral index and curvature are used, as opposed to the flux and uncertainty on the flux.\\

\begin{figure}[h]
%\centering
\includegraphics[width=\onepic\textwidth]{plots/signifcurvvsspecind2.pdf}
\includegraphics[width=\onepic\textwidth]{plots/fluxvsunc.pdf}
\caption{Separation of AGNs and PSRs from the 3FGL catalog based on different features}
\label{fig:corr}
\end{figure}




These importances were found to be consistent for various different algorithm parameters. So while the value might change a bit for different tree architechtures, for instance, the importances of these features were still pronounced. \\
\subsection{Comparison of the classification algorithms}



\subsubsection{Random Forests}


\begin{figure}[h]
%\centering
\includegraphics[width=\twopicsp\textwidth]{plots/assocvstrees} \\
\includegraphics[width=\twopicsp\textwidth]{plots/6md_trees_weighted}
\caption{
Random Forests in Training
}
\label{fig:Maps_data}
\end{figure}

The two main parameters involved in Random Forests are the number of trees and the maximum depth of the trees involved. Figures below shows one instance of the accuracy as a function of maximum depth when the number of trees was kept constant, and as a function of number of trees when the maximum depth was kept constant. We performed 100 runs for different numbers of trees (also called esimators in the code) and for the maximum depth. In the case of training, we found no significant differences when using weights for our data set. However, we chose to use a inversly weighted dataset since that allows us to generalize the algorithm more. The influence of weights will also be discussed more in the case of unassociated data, in the next section. \\

After running tests on various architectures we chose a Random Forest with 20 trees to avoid over-fitting, and a maximum depth of 12. \\

Since Random forests are based on decision trees, they allow us to characterize feature importances based on how helpful a feature was to split a tree. In our case, using all 10 features discussed above we found the following feature importances for two architectures of random forests with similar accuracies of 97.5:\\


\begin{table}[!h]
    \tiny
    \centering
    \renewcommand{\tabcolsep}{1mm}
\renewcommand{\arraystretch}{1}

    \begin{tabular}{|c|c|c|}
    \hline
    Feature Name&  RF (20,12)& RF (50,6)\\
    \hline
    Flux Density& 0 & 0        \\
    \hline
    Unc Energy Flux100& 0     & 0 \\
    \hline %\midrule   -> aakash do you mean this?
   Spectral Index & 0.13     &   0.17 \\
    \hline %\midrule   -> aakash do you mean this?
    Significant curvature& 0.33 &0.29  \\
    \hline
   var&  0.13   &  0.11  \\
    \hline %\midrule   -> aakash do you mean this?
    hr12& 0.07 &0.06 \\
    \hline
     hr23& 0.06 &0.05 \\
    \hline
    hr34& 0.05 &0.05 \\
    \hline
   hr45& 0.20 &0.23 \\
    \hline
    GLAT&0.04&0.04\\
    \hline
    \end{tabular}

    \caption{Feature importances for different Algorithm}
    \label{tab:feat_imp}
\end{table}

Our hypothesis about feature importances turned out to be correct, as curvature, variability, and spectral index were the most important features. The last hardness ratio was also seen to be quite important, most probably reflecting the end of the spectrum where the AGNs and PSRs shift from each other.\\

To see the difference in the results of models we used, we plotted the classification domains of the algorithms. \\
\begin{figure}[h]
%\centering
\includegraphics[width=\onepic\textwidth]{plots/classdom_rf_200,12.pdf}
\includegraphics[width=\onepic\textwidth]{plots/classdom_rf_20,2.pdf}
\caption{Classification Domains of Random Forests
}
\label{fig:Maps_data}
\end{figure}


\subsubsection{Neural Networks}

In the case of neural networks we were concerned with the number of epochs that one would need to tweak, along with a dependence on the number of neurons in the hidden layers. A final improvement involved checking whether multiple hidden layers would actually add to such a classification algorithm or not.\\
As can be seen in the figure, with the specific stochastic gradient method ADAM, the number of epochs required are higher since ADAM converges slow. However, after around 200 epochs the accuracy saturates. \\

\begin{figure}[h]
%\centering
\includegraphics[width=\onepic\textwidth]{plots/iter1}
\caption{
Dependence on epochs for different solvers and activation function
}
\label{fig:Maps_data}
\end{figure}

In order to fasten our calculations, we tried the lbfgs solver instead. This is much faster than ADAM, and converges easily for small data-sets. As can be seen, the results are better even for small epochs, and the accuracy saturates earlier. Therefore, we decided to chose lbfgs solver, along with 200 epochs for training, and tanh activation function. \\
\begin{figure}[h]
%\centering
\includegraphics[width=\onepic\textwidth]{plots/iter2.pdf}
\caption{
Neural Network
}
\label{fig:Maps_data}
\end{figure}

The next step was to see the effect of multiple hidden layers and number of neurons on the accuracy. In this case we found no clear dependence, and it seems as though the network doesn't overtraiin even for a higher number of neurons. This could be explained by the less number of iterations. Higher number of hidden layers added nothing more to the accuracy and serve to only over-fit the data.\\


\begin{figure}[h]
%\centerin
\includegraphics[width=\twopicsp\textwidth]{plots/neurons3.pdf}
%\includegraphics[width=\twopicsp\textwidth]{plots/epochsvsscore2_10seeds.pdf}
\caption{Dependence of accuracy on number of neurons for different models}
\label{fig:Maps_data}
\end{figure}

The classification Domains for Neural Networks look different than the tree based model of Random Forests.\\
\begin{figure}[h]
%\centerin
\includegraphics[width=\twopicsp\textwidth]{plots/classdom_nn_200_adam_tanh_5.pdf}
%\includegraphics[width=\twopicsp\textwidth]{plots/epochsvsscore2_10seeds.pdf}
\caption{Classification Domains for Neural Networks}
\label{fig:Maps_data}
\end{figure}

\subsection{Boosted Decision Trees}
Boosted Decision Trees or Gradient boosting algorithms are similar to Random Forests, having number of trees and maximum depth as parameters. However, here we also have the learning rate which specifies how fast (or slow) a model learns. This parameter is quite important, as a high value will converge faster but under-fit, whereas a lower value will over-fit. Therefore, we first chose an architecture of (50,6) and (20,6) to look at the dependence on number of trees. Here we found that low learning rate is preferred, and the maximum accuracy is reached at around 0.1-0.3. After a maximum the accuracy falls drastically since the BDT doesn't have enough time to learn properly. However, after a certain maximum depth, this doesn't hold true anymore irrespective of the number of trees. In this case, the BDT is complex enough that it learns fast and the accuracy doesn't fall even for higher learning rates. The figures below illustrate this.\\



\begin{figure}[h]
%\centerin
\includegraphics[width=\twopicsp\textwidth]{plots/lr.pdf}
\includegraphics[width=\twopicsp\textwidth]{plots/lr3.pdf}
\caption{Dependence of BDT accuracy on learning rate with number of trees and maximum depth kept constant respectively}
\label{fig:Maps_data}
\end{figure}


Choosing a learning rate of 0.3, we find for 20 and 50 trees, that as one increases the maximum depth, the accuracy of the architecture decreases, until a limit is reached.\\
\begin{figure}[h]
%\centerin
\includegraphics[width=\twopicsp\textwidth]{plots/assoc_complex.pdf}
%\includegraphics[width=\twopicsp\textwidth]{plots/epochsvsscore2_10seeds.pdf}
\caption{Dependence of BDT accuracy on maximum depth}
\label{fig:Maps_data}
\end{figure}

Boosted Decision Trees again show a similar effect as Random forests in their classification, and have a significant difference when the model complexity is changed.\\


\begin{figure}[h]
%\centerin
\includegraphics[width=\twopicsp\textwidth]{plots/classdom_bdt_100_3.pdf}
\includegraphics[width=\twopicsp\textwidth]{plots/classdom_bdt_100_15.pdf}
\caption{Classification Domains for BDT}
\label{fig:Maps_data}
\end{figure}
\subsection{Logistic Regression}

Logisitc Regression parameters include maximum iterations, tolerance, and a regularization parameter. In the figure below on can see that iterations change the accuracy only for the saga solver, and both lbfgs and liblinear are unchanged. The latter are faster algorithms good for smaller datasets like ours. Furthermore for our data set there is no strict dependence on the tolerance and after a threshold vale the regularization also produces the same result.\\

\begin{figure}[h]
%\centerin
\includegraphics[width=\twopicsp\textwidth]{plots/solver.pdf}
\includegraphics[width=\twopicsp\textwidth]{plots/tol.png}
\caption{Dependence of Logistic Regression accuracy on iterations (top), and on the regularization parameter (bottom)	}
\label{fig:Maps_data}
\end{figure}

