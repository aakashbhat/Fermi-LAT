\section{3-Class Classification}
In this section we discuss the multi-class method\footnotemark\footnotetext{Multi here refers to any non-binary number of classes}. Instead of only using sources which had AGN and PSR as their classification we added 108 sources in the 3FGL catalog which have a classification not falling under AGN or PSR. We gave them a classification of 'OTHER'. We ran our codes again, first to find the optimum parameters as was done in section 2. The only differences here are that we took the cosine of the galactic longitude and the oversampling was done with the square root of the fraction of AGNs with the PSR or OTHER classifications. Therefore, for oversampling, the number of PSRs and OTHERs were multiplied by $\sqrt{\frac{\text{|AGN|}}{\text{|PSR|}}}$ and $\sqrt{\frac{\text{|AGN|}}{\text{|OTHER|}}}$ respectively. We chose these weights so as to not oversample and bias our methods too much. This is effectively clear for instance if we would go to even higher classes and the number of data points shall become smaller. 

Figure \ref{fig:tree_multi} and \ref{fig:nets_multi} shows our results.The tree based methods show the same behaviour as before and therefore we keep the same parameters for our final models. For Neural Networks and Logistic regression the behaviour is also the same but the optimal parameters are found to be 600 and 500 epochs instead of 300 and 200 respectively. We keep the same number of neurons as before for the neural network.


\begin{figure}[h]
\center
%\hspace*{-1cm}
\includegraphics[width=0.5\textwidth]{plots/rf_train_multi.pdf}\\
%\hspace*{-1cm}
\includegraphics[width=0.5\textwidth]{plots/bdt_train_multi.pdf}
\includegraphics[width=0.5\textwidth]{plots/lr_train_multi.pdf}
\caption{Testing scores for classification methods of RF, BDT and LR.}
\label{fig:tree_multi}
\end{figure}
\begin{figure}[h]
\center
%\hspace*{-1cm}
\includegraphics[width=0.5\textwidth]{plots/nn_epoch_train_multi.pdf}\\
%\hspace*{-1cm}
\includegraphics[width=0.5\textwidth]{plots/nn_neuron_train_multi.pdf}
\caption{Testing scores for classification methods of NN for a set number of Epochs and a set number of Neurons.}
\label{fig:nets_multi}
\end{figure}

The final accuracies of our chosen model for 1000 runs are given below in table \ref{tab:selected_algs_multi} for 3FGL and table \ref{tab:selected_algs_4fgl_multi} for 4FGL-DR2. For 3FGL the accuracies are always around 93-94 \%.
\begin{table}[!h]
\hspace{-0.2cm}
\resizebox{0.47\textwidth}{!}{
    \tiny
  \centering
    \renewcommand{\tabcolsep}{0.4mm}
\renewcommand{\arraystretch}{1.6}

%\hspace{-3mm}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    Algorithm&Parameters &  Testing&Std. Dev.& Comparison with \\
    & & Accuracy & & 4FGL DR-2 Accuracy \\
    \hline
    RF & 50 trees, max depth 6  &93.96&0.85& 88.92  \\
    RF\_O &   &94.38&0.76& 87.87 \\
    \hline %\midrule   -> aakash do you mean this?
    BDT & 100 trees, max depth 2    &   93.72&0.83&87.20 \\
%    \hline %\midrule   -> aakash do you mean this?
%    BDT & 200 trees, max depth 2    &   95.8  \\
    BDT\_O &     &   93.83&0.80& 86.69 \\
    \hline
    NN & 600 epochs, 11 neurons, LBFGS & 93.17&1.05& 86.39\\
    NN\_O &&  92.51 &1.34& 83.97\\
    \hline
    LR & 500 iterations, LBFGS solver & 93.93&0.88& 87.65 \\
    LR\_O &   &93.01&0.96& 85.59 \\
    \hline
     
    \end{tabular}}
    \vspace{2mm}
    \caption{Testing accuracy of the 4 selected algorithms for classification of 3FGL sources and comparison with associations in the 4FGL-DR2 catalog. 
    ``\_O'' denotes training with oversampling.}
    \label{tab:selected_algs_multi}
\end{table}
\begin{table}[!h]
\hspace{-0.2cm}
\resizebox{0.47\textwidth}{!}{
    \tiny
  \centering
    \renewcommand{\tabcolsep}{0.4mm}
\renewcommand{\arraystretch}{1.6}
%\hspace{-3mm}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Algorithm&Parameters &  Testing&Std. Dev.\\
    & & Accuracy &  \\
    \hline
    RF & 50 trees, max depth 6  &92.91&0.66\\
    RF\_O &   &92.83&0.63 \\
    \hline %\midrule   -> aakash do you mean this?
    BDT & 100 trees, max depth 2    &   92.51&0.67 \\
%    \hline %\midrule   -> aakash do you mean this?
%    BDT & 200 trees, max depth 2    &   95.8  \\
    BDT\_O &     &   92.27&0.67 \\
    \hline
    NN & 600 epochs, 16 neurons, LBFGS & 91.86&0.72\\
    NN\_O &  & 90.26&0.83\\
    \hline
    LR & 1000 iterations, LBFGS solver & 92.63&0.67 \\
    LR\_O &  &92.22&0.69\\
    \hline
     
    \end{tabular}}
    \vspace{2mm}
    \caption{Testing accuracy of the 4 selected algorithms for classification of 4FGL-DR2 sources. 
    ``\_O'' denotes training with oversampling.}
    \label{tab:selected_algs_4fgl_multi}
\end{table}